---
title: 机器学习中几种常见的梯度更新算法
date: 2023-03-24 23:00:00 +0800
categories: [理论, 机器学习]
tags: [运筹优化, 机器学习]     # TAG names should always be lowercase
math: true
---


最近在了解机器学习算法的知识，看了一些关于梯度更新的文章，在这里想把几种常见的梯度更新的算法介绍一下，顺便加深一下自己的印象。对于确定迭代算法的迭代方向，一般是计算其当前梯度乘上步长的负值进行迭代，以此迭代求出全局最小值。此称为梯度下降法，基本公式如下：
$$x_{n+1}=x_n-a\nabla f(x_n)$$
这种迭代方法的主要原理是由于函数在固定一点的梯度是其下降最快的地方，所以以此方向进行下一次迭代，但是由于迭代的步长 $a$ 是固定的，有时会出现步长过大以致于无法进入全局最小值或者步长过小以致于将局部最小值判断为全局最小值，所以说步长也应该是需要迭代更新的。下面是几种迭代步长的算法：

# Exact Line search

Exact Line search的算法公式如下：
$$x_{n+1}=x_n-a_n\nabla f(x_n)$$
$$a_n={\arg\min}\limits{s}f(x_n-s\nabla f(x_n))$$
这种更新步长的方式实际上是在每次迭代时选择能让迭代后的函数值最小的步长，所以这种方法的效果是最好的，但恰恰因为此，这种方法的计算量是最复杂的，相当于每次迭代都要解一个优化问题，所以这种方法仅仅理论上可行，现实中没人使用它。

# Backtracking Line Search

Backtracking Line Search的算法如下：
$$x_{n+1}=x_n-a_n\nabla f(x_n)$$
$$repeat\quad a:=\beta a\quad if\quad f(x_n+a\nabla f(x_n))<f(x_n)-\alpha a||\nabla f(x_n)||^2 x$$
其中 $\alpha$ 取值为(0,0.5)，$\beta$ 取值为(0,1)。这种方法通过自适应迭代调整每一步的步长，以此更快更好的收敛到全局最小值。其步长更新的原理为从大到小搜索，相当于牺牲了Exact Line search的精确性，通过迭代代替优化，极大的降低了计算的复杂性，又能保证结果在一个允许的区间内。

# Barzilai–Borwein



# Momentum



# AdaGrad



# Adam




